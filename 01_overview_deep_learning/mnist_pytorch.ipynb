{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define training and inference routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])), batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=1000, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define optimizer and run training epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "model = Net()\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=0.5)\n",
    "\n",
    "# scheduler = StepLR(optimizer, step_size=1, gamma=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.318902\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 1.260487\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 0.845640\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 0.910280\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.390269\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.332739\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.448877\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 0.298393\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.503042\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 0.223934\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.408854\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.164622\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.169872\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.469457\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.381625\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.340721\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.175661\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.176566\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.163720\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.197197\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.358346\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.134378\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.132333\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.267857\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.205452\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.124313\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.105835\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.111669\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.105846\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.077849\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.048854\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.040286\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.128138\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.257019\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.100136\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.137551\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.027502\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.297030\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.137334\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.076728\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.121437\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.131006\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.057057\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.175813\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.082588\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.186991\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.157580\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.055714\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.081465\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.062140\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.117909\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.105527\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.089232\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.174845\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.193626\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.125986\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.092158\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.062837\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.045952\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.094160\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.179968\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.034778\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.071585\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.092989\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.207097\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.225864\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.171925\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.138131\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.090487\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.142658\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.074566\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.263405\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.088093\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.047315\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.061875\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.116237\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.256587\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.091231\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.091463\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.270116\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.101415\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.045957\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.041018\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.047776\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.258058\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.107430\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.148437\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.151625\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.066304\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.110612\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.037599\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.032077\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.057487\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.077352\n",
      "\n",
      "Test set: Average loss: 0.0521, Accuracy: 9832/10000 (98%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.087238\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.104193\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.322711\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.136174\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.156950\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.082425\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.233045\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.046715\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.072223\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.103762\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.031990\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.033122\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.201769\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.073957\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.016892\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.196654\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.099060\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.178196\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.122408\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.056701\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.048457\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.109592\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.064588\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.228939\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.037534\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.069423\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.161837\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.097324\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.048843\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.095035\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.088634\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.032306\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.145361\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.015991\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.132456\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.074081\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.077504\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.105257\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.012870\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.060334\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.045569\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.015087\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.050177\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.129137\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.043021\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.048817\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.119929\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.097498\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.027618\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.202292\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.129128\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.217096\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.058108\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.071650\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.075060\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.079639\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.050170\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.012579\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.115815\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.126229\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.120416\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.066206\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.020446\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.034014\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.058721\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.061586\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.060029\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.039625\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.091260\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.011014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.034660\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.024653\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.081592\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.074604\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.044495\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.098270\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.073216\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.014415\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.004895\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.025325\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.100192\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.115016\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.136716\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.074478\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.010072\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.051746\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.123856\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.166777\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.003768\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.340687\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.180309\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.043444\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.012978\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.062735\n",
      "\n",
      "Test set: Average loss: 0.0421, Accuracy: 9862/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 3):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run inference on trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x133ad9410>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAOQElEQVR4nO3df4xV9ZnH8c8jSxMREgGVoMW1IiSgRooTsnHN2gmhmSUm0AQN/LHBtHH6ByYlMbGkaIohNbqi+yeGBgKSrtiIWlLWpQM2q41KHIjLLxf8BcERmQh/FFADwtM/5tAdcc73Dvece8+ded6vZDL3nme+9zy5+uGce37cr7m7AAx/V1TdAIDmIOxAEIQdCIKwA0EQdiCIf2jmysyMQ/9Ag7m7DbS80JbdzDrM7KCZfWhmy4q8FoDGsnrPs5vZCEmHJM2R9KmkdyUtcvcDiTFs2YEGa8SWfZakD939Y3c/K2mTpHkFXg9AAxUJ+w2SjvZ7/mm27FvMrNPMus2su8C6ABTU8AN07r5G0hqJ3XigSkW27D2SJvV7/v1sGYAWVCTs70qaYmY/MLPvSVooaUs5bQEoW9278e7+jZk9JGmbpBGS1rn7/tI6A1Cquk+91bUyPrMDDdeQi2oADB2EHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgTR1Cmb0RgjR47Mrc2cOTM59tFHH03W586dm6xfcUV6e3HhwoXcWldXV3JsR0dHso7Lw5YdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4JgFtch4JZbbknWn3322dxarfPktRw4cCBZP3/+fLI+adKk3NpXX32VHHvXXXcl60ePHk3Wo8qbxbXQRTVmdljSKUnnJX3j7m1FXg9A45RxBV27u39RwusAaCA+swNBFA27S/qTme0ys86B/sDMOs2s28y6C64LQAFFd+PvdvceM7tOUpeZ/Z+7v9H/D9x9jaQ1EgfogCoV2rK7e0/2u1fSK5JmldEUgPLVHXYzu8rMxlx8LOnHkvaV1RiAchXZjZ8g6RUzu/g6/+nu/11KV8Hcd999yfpTTz2VrN944425tRMnTiTHLl26NFnfvHlzsn727Nlk/ZFHHsmttbe3J8eOGDEiWcflqTvs7v6xpDtK7AVAA3HqDQiCsANBEHYgCMIOBEHYgSC4xbUFrF27Nll/4IEHkvXly5fn1jZt2pQce/jw4WQdQ0/eLa5s2YEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCKZsbgFHjhwpND51G+nbb7+dHNvK59lvvfXWZL1W72fOnCmxm6GPLTsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBMH97C2grS09+e3jjz+erHd0dOTWap3DX7lyZbL+2muvJesPP/xwsp7qbfr06cmxX3/9dbK+YMGCZL1W78MV97MDwRF2IAjCDgRB2IEgCDsQBGEHgiDsQBCcZx8Cxo8fn6wfPHgwt3b11VeX3c63ZFN25/ryyy9za11dXcmxq1atStbfeuutZD2qus+zm9k6M+s1s339lo0zsy4z+yD7PbbMZgGUbzC78eslXXoZ1DJJO9x9iqQd2XMALaxm2N39DUknL1k8T9KG7PEGSfNL7gtAyer9DroJ7n4se/y5pAl5f2hmnZI661wPgJIU/sJJd/fUgTd3XyNpjcQBOqBK9Z56O25mEyUp+91bXksAGqHesG+RtDh7vFjSH8ppB0Cj1DzPbmYvSPqRpGskHZf0a0mvSvq9pBslHZF0v7tfehBvoNdiN74BbrvtttzaunXrkmNnzpxZaN3bt29P1lesWJFbe+eddwqtGwPLO89e8zO7uy/KKc0u1BGApuJyWSAIwg4EQdiBIAg7EARhB4JgyuZhYMqUKbm1qVOnNnTd7e3tyfrrr7+eW+PUW3OxZQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIPgq6SHguuuuS9Y/+uij3NqVV16ZHPvSSy8l6xs3bkzWt2zZkqyfO3cut7Z8+fLk2GeeeSZZx8CYshkIjrADQRB2IAjCDgRB2IEgCDsQBGEHguB+9iGg1j3jo0aNqvu1t23blqxv3bo1Wb/33nuT9dR5/CVLliTHnjyZ/nby559/Plk/f/58sh4NW3YgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIL72YeAvXv3JuvTpk2r+7VnzJiRrO/bt6/u15ak+fPn59bWr1+fHDt69Ohk/Y477kjW9+/fn6wPV3Xfz25m68ys18z29Vu2wsx6zOy97Gdumc0CKN9gduPXS+oYYPl/uPuM7Oe/ym0LQNlqht3d35CUvm4RQMsrcoDuITPbk+3mj837IzPrNLNuM+susC4ABdUb9tWSJkuaIemYpNxvBnT3Ne7e5u5tda4LQAnqCru7H3f38+5+QdJvJc0qty0AZasr7GY2sd/Tn0gqdn4GQMPVPM9uZi9I+pGkayQdl/Tr7PkMSS7psKSfu/uxmivjPPuA2trSn3B27tyZrKf+G+7evTs5dvbs2cn6qVOnkvUiHnzwwWR99erVyfqRI0eS9cmTJ192T8NB3nn2ml9e4e6LBli8tnBHAJqKy2WBIAg7EARhB4Ig7EAQhB0Igq+SHgYOHTqUW7v//vuTYxt5aq2WTZs2JesdHQPdf/X/5s2bl6wvXrw4t7Zhw4bk2OGILTsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBMF59hbQ3V3sG7vGjBlTUifNVesc/xNPPJGsz5kzJ1m/9tprL7un4YwtOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EwXn2IWDlypXJ+mOPPZZbmzUrPX9HT09Psn7u3LlkvZF27dqVrJ85cyZZv+eee3Jrq1atqqunoYwtOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EUXPK5lJXxpTNdRk/fnyy/uabb+bWpk6dmhy7devWZH379u3J+osvvpis9/b2JutFfPbZZ3WPvf7660vspLXkTdlcc8tuZpPM7M9mdsDM9pvZL7Ll48ysy8w+yH6PLbtpAOUZzG78N5Iedvfpkv5J0hIzmy5pmaQd7j5F0o7sOYAWVTPs7n7M3Xdnj09Jel/SDZLmSbo4h84GSfMb1SSA4i7r2ngzu0nSDyXtlDTB3Y9lpc8lTcgZ0ymps/4WAZRh0EfjzWy0pM2Slrr7X/vXvO8o34AH39x9jbu3uXtboU4BFDKosJvZSPUF/Xfu/nK2+LiZTczqEyU17rArgMJqnnozM1PfZ/KT7r603/KnJZ1w9yfNbJmkce7+SI3X4tRbA6S+Svrpp59Ojl24cGGyPnr06GR9z549yfqyZfnHbU+cOJEcW+sWV069DSzv1NtgPrP/s6R/k7TXzN7Llv1K0pOSfm9mP5N0RFJ6InAAlaoZdnf/i6QB/6WQNLvcdgA0CpfLAkEQdiAIwg4EQdiBIAg7EAS3uAY3f376loaNGzcm66NGjUrWU/9/nT59Ojn2k08+SdanTZuWrJ88eTK3FvE8O1t2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCKZuDe/XVV5P1m2++OVlvb29P1u+8887c2oIFC5Jjb7/99mS9lueee67Q+OGGLTsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBMH97MAww/3sQHCEHQiCsANBEHYgCMIOBEHYgSAIOxBEzbCb2SQz+7OZHTCz/Wb2i2z5CjPrMbP3sp+5jW8XQL1qXlRjZhMlTXT33WY2RtIuSfPVNx/7aXdfNeiVcVEN0HB5F9UMZn72Y5KOZY9Pmdn7km4otz0AjXZZn9nN7CZJP5S0M1v0kJntMbN1ZjY2Z0ynmXWbWXehTgEUMuhr481stKT/kfQbd3/ZzCZI+kKSS1qpvl39n9Z4DXbjgQbL240fVNjNbKSkP0ra5u7PDlC/SdIf3f22Gq9D2IEGq/tGGDMzSWslvd8/6NmBu4t+Imlf0SYBNM5gjsbfLelNSXslXcgW/0rSIkkz1Lcbf1jSz7ODeanXYssONFih3fiyEHag8bifHQiOsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EETNL5ws2ReSjvR7fk22rBW1am+t2pdEb/Uqs7d/zCs09X7276zcrNvd2yprIKFVe2vVviR6q1ezemM3HgiCsANBVB32NRWvP6VVe2vVviR6q1dTeqv0MzuA5ql6yw6gSQg7EEQlYTezDjM7aGYfmtmyKnrIY2aHzWxvNg11pfPTZXPo9ZrZvn7LxplZl5l9kP0ecI69inpriWm8E9OMV/reVT39edM/s5vZCEmHJM2R9KmkdyUtcvcDTW0kh5kdltTm7pVfgGFm/yLptKTnL06tZWb/Lumkuz+Z/UM51t1/2SK9rdBlTuPdoN7yphl/QBW+d2VOf16PKrbssyR96O4fu/tZSZskzaugj5bn7m9IOnnJ4nmSNmSPN6jvf5amy+mtJbj7MXffnT0+JeniNOOVvneJvpqiirDfIOlov+efqrXme3dJfzKzXWbWWXUzA5jQb5qtzyVNqLKZAdScxruZLplmvGXeu3qmPy+KA3Tfdbe7z5T0r5KWZLurLcn7PoO10rnT1ZImq28OwGOSnqmymWya8c2Slrr7X/vXqnzvBuirKe9bFWHvkTSp3/PvZ8tagrv3ZL97Jb2ivo8dreT4xRl0s9+9Fffzd+5+3N3Pu/sFSb9Vhe9dNs34Zkm/c/eXs8WVv3cD9dWs962KsL8raYqZ/cDMvidpoaQtFfTxHWZ2VXbgRGZ2laQfq/Wmot4iaXH2eLGkP1TYy7e0yjTeedOMq+L3rvLpz9296T+S5qrviPxHkpZX0UNOXzdL+t/sZ3/VvUl6QX27defUd2zjZ5LGS9oh6QNJ2yWNa6HeNqpvau896gvWxIp6u1t9u+h7JL2X/cyt+r1L9NWU943LZYEgOEAHBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0H8DY3KpF7F9NC8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(example_data[0][0], cmap='gray', interpolation='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(example_data).data.max(1, keepdim=True)[1][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
