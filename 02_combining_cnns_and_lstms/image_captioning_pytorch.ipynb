{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linux\n",
    "!apt-get install wget\n",
    "# mac\n",
    "!brew install wget\n",
    " \n",
    "# create a data directory\n",
    "!mkdir data_dir\n",
    " \n",
    "# download images and annotations to the data directory\n",
    "!wget http://msvocds.blob.core.windows.net/annotations-1-0-3/captions_train-val2014.zip -P ./data_dir/\n",
    "!wget http://images.cocodataset.org/zips/train2014.zip -P ./data_dir/\n",
    "!wget http://images.cocodataset.org/zips/val2014.zip -P ./data_dir/\n",
    "    \n",
    "# extract zipped images and annotations and remove the zip files\n",
    "!unzip ./data_dir/captions_train-val2014.zip -d ./data_dir/\n",
    "!rm ./data_dir/captions_train-val2014.zip\n",
    "!unzip ./data_dir/train2014.zip -d ./data_dir/\n",
    "!rm ./data_dir/train2014.zip \n",
    "!unzip ./data_dir/val2014.zip -d ./data_dir/ \n",
    "!rm ./data_dir/val2014.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import pickle\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "from pycocotools.coco import COCO\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.nn.utils.rnn import pack_padded_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Vocab(object):\n",
    "    \"\"\"Simple vocabulary wrapper.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.w2i = {}\n",
    "        self.i2w = {}\n",
    "        self.index = 0\n",
    " \n",
    "    def __call__(self, token):\n",
    "        if not token in self.w2i:\n",
    "            return self.w2i['<unk>']\n",
    "        return self.w2i[token]\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.w2i)\n",
    "    def add_token(self, token):\n",
    "        if not token in self.w2i:\n",
    "            self.w2i[token] = self.index\n",
    "            self.i2w[self.index] = token\n",
    "            self.index += 1\n",
    "\n",
    "def build_vocabulary(json, threshold):\n",
    "    \"\"\"Build a simple vocabulary wrapper.\"\"\"\n",
    "    coco = COCO(json)\n",
    "    counter = Counter()\n",
    "    ids = coco.anns.keys()\n",
    "    for i, id in enumerate(ids):\n",
    "        caption = str(coco.anns[id]['caption'])\n",
    "        tokens = nltk.tokenize.word_tokenize(caption.lower())\n",
    "        counter.update(tokens)\n",
    " \n",
    "        if (i+1) % 1000 == 0:\n",
    "            print(\"[{}/{}] Tokenized the captions.\".format(i+1, len(ids)))\n",
    " \n",
    "    # If the word frequency is less than 'threshold', then the word is discarded.\n",
    "    tokens = [token for token, cnt in counter.items() if cnt >= threshold]\n",
    " \n",
    "    # Create a vocab wrapper and add some special tokens.\n",
    "    vocab = Vocab()\n",
    "    vocab.add_token('<pad>')\n",
    "    vocab.add_token('<start>')\n",
    "    vocab.add_token('<end>')\n",
    "    vocab.add_token('<unk>')\n",
    " \n",
    "    # Add the words to the vocabulary.\n",
    "    for i, token in enumerate(tokens):\n",
    "        vocab.add_token(token)\n",
    "    return vocab\n",
    " \n",
    "vocab = build_vocabulary(json='data_dir/annotations/captions_train2014.json', threshold=4)\n",
    "vocab_path = './data_dir/vocabulary.pkl'\n",
    "with open(vocab_path, 'wb') as f:\n",
    "    pickle.dump(vocab, f)\n",
    "print(\"Total vocabulary size: {}\".format(len(vocab)))\n",
    "print(\"Saved the vocabulary wrapper to '{}'\".format(vocab_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## resize images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def reshape_image(image, shape):\n",
    "    \"\"\"Resize an image to the given shape.\"\"\"\n",
    "    return image.resize(shape, Image.ANTIALIAS)\n",
    " \n",
    "def reshape_images(image_path, output_path, shape):\n",
    "    \"\"\"Reshape the images in 'image_path' and save into 'output_path'.\"\"\"\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    " \n",
    "    images = os.listdir(image_path)\n",
    "    num_im = len(images)\n",
    "    for i, im in enumerate(images):\n",
    "        with open(os.path.join(image_path, im), 'r+b') as f:\n",
    "            with Image.open(f) as image:\n",
    "                image = reshape_image(image, shape)\n",
    "                image.save(os.path.join(output_path, im), image.format)\n",
    "        if (i+1) % 100 == 0:\n",
    "            print (\"[{}/{}] Resized the images and saved into '{}'.\"\n",
    "                   .format(i+1, num_im, output_path))\n",
    "\n",
    "image_path = './data_dir/train2014/'\n",
    "output_path = './data_dir/resized_images/'\n",
    "image_shape = [256, 256]\n",
    "reshape_images(image_path, output_path, image_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## instantiate data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCocoDataset(data.Dataset):\n",
    "    \"\"\"COCO Custom Dataset compatible with torch.utils.data.DataLoader.\"\"\"\n",
    "    def __init__(self, data_path, coco_json_path, vocabulary, transform=None):\n",
    "        \"\"\"Set the path for images, captions and vocabulary wrapper.\n",
    "        \n",
    "        Args:\n",
    "            root: image directory.\n",
    "            json: coco annotation file path.\n",
    "            vocab: vocabulary wrapper.\n",
    "            transform: image transformer.\n",
    "        \"\"\"\n",
    "        self.root = data_path\n",
    "        self.coco_data = COCO(coco_json_path)\n",
    "        self.indices = list(self.coco_data.anns.keys())\n",
    "        self.vocabulary = vocabulary\n",
    "        self.transform = transform\n",
    " \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Returns one data pair (image and caption).\"\"\"\n",
    "        coco_data = self.coco_data\n",
    "        vocabulary = self.vocabulary\n",
    "        annotation_id = self.indices[idx]\n",
    "        caption = coco_data.anns[annotation_id]['caption']\n",
    "        image_id = coco_data.anns[annotation_id]['image_id']\n",
    "        image_path = coco_data.loadImgs(image_id)[0]['file_name']\n",
    " \n",
    "        image = Image.open(os.path.join(self.root, image_path)).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    " \n",
    "        # Convert caption (string) to word ids.\n",
    "        word_tokens = nltk.tokenize.word_tokenize(str(caption).lower())\n",
    "        caption = []\n",
    "        caption.append(vocabulary('<start>'))\n",
    "        caption.extend([vocabulary(token) for token in word_tokens])\n",
    "        caption.append(vocabulary('<end>'))\n",
    "        ground_truth = torch.Tensor(caption)\n",
    "        return image, ground_truth\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    " \n",
    " \n",
    "def collate_function(data_batch):\n",
    "    \"\"\"Creates mini-batch tensors from the list of tuples (image, caption).\n",
    "    \n",
    "    We should build custom collate_fn rather than using default collate_fn, \n",
    "    because merging caption (including padding) is not supported in default.\n",
    "    Args:\n",
    "        data: list of tuple (image, caption). \n",
    "            - image: torch tensor of shape (3, 256, 256).\n",
    "            - caption: torch tensor of shape (?); variable length.\n",
    "    Returns:\n",
    "        images: torch tensor of shape (batch_size, 3, 256, 256).\n",
    "        targets: torch tensor of shape (batch_size, padded_length).\n",
    "        lengths: list; valid length for each padded caption.\n",
    "    \"\"\"\n",
    "    # Sort a data list by caption length (descending order).\n",
    "    data_batch.sort(key=lambda d: len(d[1]), reverse=True)\n",
    "    imgs, caps = zip(*data_batch)\n",
    " \n",
    "    # Merge images (from tuple of 3D tensor to 4D tensor).\n",
    "    imgs = torch.stack(imgs, 0)\n",
    " \n",
    "    # Merge captions (from tuple of 1D tensor to 2D tensor).\n",
    "    cap_lens = [len(cap) for cap in caps]\n",
    "    tgts = torch.zeros(len(caps), max(cap_lens)).long()\n",
    "    for i, cap in enumerate(caps):\n",
    "        end = cap_lens[i]\n",
    "        tgts[i, :end] = cap[:end]        \n",
    "    return imgs, tgts, cap_lens\n",
    " \n",
    "def get_loader(data_path, coco_json_path, vocabulary, transform, batch_size, shuffle, num_workers):\n",
    "    \"\"\"Returns torch.utils.data.DataLoader for custom coco dataset.\"\"\"\n",
    "    # COCO caption dataset\n",
    "    coco_dataser = CustomCocoDataset(data_path=data_path,\n",
    "                       coco_json_path=coco_json_path,\n",
    "                       vocabulary=vocabulary,\n",
    "                       transform=transform)\n",
    "    \n",
    "    # Data loader for COCO dataset\n",
    "    # This will return (images, captions, lengths) for each iteration.\n",
    "    # images: a tensor of shape (batch_size, 3, 224, 224).\n",
    "    # captions: a tensor of shape (batch_size, padded_length).\n",
    "    # lengths: a list indicating valid length for each caption. length is (batch_size).\n",
    "    custom_data_loader = torch.utils.data.DataLoader(dataset=coco_dataser, \n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=shuffle,\n",
    "                                              num_workers=num_workers,\n",
    "                                              collate_fn=collate_function)\n",
    "    return custom_data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, embedding_size):\n",
    "        \"\"\"Load the pretrained ResNet-152 and replace top fc layer.\"\"\"\n",
    "        super(CNNModel, self).__init__()\n",
    "        resnet = models.resnet152(pretrained=True)\n",
    "        module_list = list(resnet.children())[:-1]      # delete the last fc layer.\n",
    "        self.resnet_module = nn.Sequential(*module_list)\n",
    "        self.linear_layer = nn.Linear(resnet.fc.in_features, embedding_size)\n",
    "        self.batch_norm = nn.BatchNorm1d(embedding_size, momentum=0.01)\n",
    "        \n",
    "    def forward(self, input_images):\n",
    "        \"\"\"Extract feature vectors from input images.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            resnet_features = self.resnet_module(input_images)\n",
    "        resnet_features = resnet_features.reshape(resnet_features.size(0), -1)\n",
    "        final_features = self.batch_norm(self.linear_layer(resnet_features))\n",
    "        return final_features\n",
    " \n",
    " \n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_layer_size, vocabulary_size, num_layers, max_seq_len=20):\n",
    "        \"\"\"Set the hyper-parameters and build the layers.\"\"\"\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding_layer = nn.Embedding(vocabulary_size, embedding_size)\n",
    "        self.lstm_layer = nn.LSTM(embedding_size, hidden_layer_size, num_layers, batch_first=True)\n",
    "        self.linear_layer = nn.Linear(hidden_layer_size, vocabulary_size)\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "    def forward(self, input_features, capts, lens):\n",
    "        \"\"\"Decode image feature vectors and generates captions.\"\"\"\n",
    "        embeddings = self.embedding_layer(caps)\n",
    "        embeddings = torch.cat((input_features.unsqueeze(1), embeddings), 1)\n",
    "        lstm_input = pack_padded_sequence(embeddings, lens, batch_first=True) \n",
    "        hidden_variables, _ = self.lstm_layer(lstm_input)\n",
    "        model_outputs = self.linear_layer(hidden_variables[0])\n",
    "        return model_outputs\n",
    "    \n",
    "    def sample(self, input_features, lstm_states=None):\n",
    "        \"\"\"Generate captions for given image features using greedy search.\"\"\"\n",
    "        sampled_indices = []\n",
    "        lstm_inputs = input_features.unsqueeze(1)\n",
    "        for i in range(self.max_seq_len):\n",
    "            hidden_variables, lstm_states = self.lstm_layer(lstm_inputs, lstm_states)          # hiddens: (batch_size, 1, hidden_size)\n",
    "            model_outputs = self.linear_layer(hidden_variables.squeeze(1))            # outputs:  (batch_size, vocab_size)\n",
    "            _, predicted_outputs = model_outputs.max(1)                        # predicted: (batch_size)\n",
    "            sampled_indices.append(predicted_outputs)\n",
    "            lstm_inputs = self.embedding_layer(predicted_outputs)                       # inputs: (batch_size, embed_size)\n",
    "            lstm_inputs = lstm_inputs.unsqueeze(1)                         # inputs: (batch_size, 1, embed_size)\n",
    "        sampled_indices = torch.stack(sampled_indices, 1)                # sampled_ids: (batch_size, max_seq_length)\n",
    "        return sampled_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# Create model directory\n",
    "if not os.path.exists('models_dir/'):\n",
    "    os.makedirs('models_dir/')\n",
    "\n",
    "    \n",
    "# Image preprocessing, normalization for the pretrained resnet\n",
    "transform = transforms.Compose([ \n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(), \n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "\n",
    "# Load vocabulary wrapper\n",
    "with open('data_dir/vocabulary.pkl', 'rb') as f:\n",
    "    vocabulary = pickle.load(f)\n",
    "\n",
    "    \n",
    "# Build data loader\n",
    "custom_data_loader = get_loader('data_dir/resized2014', 'data_dir/annotations/captions_train2014.json', vocabulary, \n",
    "                         transform, 128,\n",
    "                         shuffle=True, num_workers=2) \n",
    "\n",
    "\n",
    "# Build the models\n",
    "encoder_model = CNNModel(256).to(device)\n",
    "decoder_model = LSTMModel(256, 512, len(vocabulary), 1).to(device)\n",
    " \n",
    "    \n",
    "# Loss and optimizer\n",
    "loss_criterion = nn.CrossEntropyLoss()\n",
    "parameters = list(decoder_model.parameters()) + list(encoder_model.linear_layer.parameters()) + list(encoder_model.batch_norm.parameters())\n",
    "optimizer = torch.optim.Adam(parameters, lr=0.001)\n",
    "\n",
    "\n",
    "# Train the models\n",
    "total_num_steps = len(custom_data_loader)\n",
    "for epoch in range(5):\n",
    "    for i, (imgs, caps, lens) in enumerate(custom_data_loader):\n",
    " \n",
    "        # Set mini-batch dataset\n",
    "        imgs = imgs.to(device)\n",
    "        caps = caps.to(device)\n",
    "        tgts = pack_padded_sequence(caps, lens, batch_first=True)[0]\n",
    " \n",
    "        # Forward, backward and optimize\n",
    "        feats = encoder_model(imgs)\n",
    "        outputs = decoder_model(feats, caps, lens)\n",
    "        loss = loss_criterion(outputs, tgts)\n",
    "        decoder_model.zero_grad()\n",
    "        encoder_model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    " \n",
    "        # Print log info\n",
    "        if i % 10 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Perplexity: {:5.4f}'\n",
    "                  .format(epoch, 5, i, total_num_steps, loss.item(), np.exp(loss.item()))) \n",
    " \n",
    "        # Save the model checkpoints\n",
    "        if (i+1) % 1000 == 0:\n",
    "            torch.save(decoder_model.state_dict(), os.path.join(\n",
    "                'models_dir/', 'decoder-{}-{}.ckpt'.format(epoch+1, i+1)))\n",
    "            torch.save(encoder_model.state_dict(), os.path.join(\n",
    "                'models_dir/', 'encoder-{}-{}.ckpt'.format(epoch+1, i+1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â predict caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_file_path = 'sample.jpg'\n",
    " \n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    " \n",
    "\n",
    "def load_image(image_file_path, transform=None):\n",
    "    img = Image.open(image_file_path).convert('RGB')\n",
    "    img = img.resize([224, 224], Image.LANCZOS)\n",
    "    \n",
    "    if transform is not None:\n",
    "        img = transform(img).unsqueeze(0)\n",
    "    \n",
    "    return img\n",
    " \n",
    "\n",
    "# Image preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "\n",
    "# Load vocabulary wrapper\n",
    "with open('data_dir/vocabulary.pkl', 'rb') as f:\n",
    "    vocabulary = pickle.load(f)\n",
    "\n",
    "\n",
    "# Build models\n",
    "encoder_model = CNNModel(256).eval()  # eval mode (batchnorm uses moving mean/variance)\n",
    "decoder_model = LSTMModel(256, 512, len(vocabulary), 1)\n",
    "encoder_model = encoder_model.to(device)\n",
    "decoder_model = decoder_model.to(device)\n",
    "\n",
    "\n",
    "# Load the trained model parameters\n",
    "encoder_model.load_state_dict(torch.load('models_dir/encoder-2-3000.ckpt'))\n",
    "decoder_model.load_state_dict(torch.load('models_dir/decoder-2-3000.ckpt'))\n",
    "\n",
    "\n",
    "# Prepare an image\n",
    "img = load_image(image_file_path, transform)\n",
    "img_tensor = img.to(device)\n",
    "\n",
    "\n",
    "# Generate an caption from the image\n",
    "feat = encoder_model(img_tensor)\n",
    "sampled_indices = decoder_model.sample(feat)\n",
    "sampled_indices = sampled_indices[0].cpu().numpy()          # (1, max_seq_length) -> (max_seq_length)\n",
    "\n",
    "\n",
    "# Convert word_ids to words\n",
    "predicted_caption = []\n",
    "for token_index in sampled_indices:\n",
    "    word = vocabulary.i2w[token_index]\n",
    "    predicted_caption.append(word)\n",
    "    if word == '<end>':\n",
    "        break\n",
    "predicted_sentence = ' '.join(predicted_caption)\n",
    "\n",
    "\n",
    "# Print out the image and the generated caption\n",
    "print (predicted_sentence)\n",
    "img = Image.open(image_file_path)\n",
    "plt.imshow(np.asarray(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
